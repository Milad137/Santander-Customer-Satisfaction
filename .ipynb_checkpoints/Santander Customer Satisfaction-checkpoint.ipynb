{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "   ### Introduction\n",
    "   ### 1. Data Preparation\n",
    "   ### 2. Feature Selection\n",
    "   ### 3. Feature Enginering\n",
    "   ### 4. Modeling\n",
    "   ### 5. Grid Search\n",
    "   ### 6. Model Selection and Hyperparameter Tuning\n",
    "   ### 7. Export Best Model for Deployment\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Provided anonymized dataset contains a large number of numeric variables. The \"TARGET\" column is the variable to predict. It equals one for unsatisfied customers and 0 for satisfied customers.\n",
    "\n",
    "The task is to predict the probability that each customer in the test set is an unsatisfied customer.\n",
    "\n",
    "File descriptions<br>\n",
    "train.csv - the training set including the target<br>\n",
    "test.csv - the test set without the target<br>\n",
    "sample_submission.csv - a sample submission file in the correct format<br>\n",
    "\n",
    "\n",
    "\n",
    "### Project Flow Chart\n",
    "![Alt text](attch/Diagram.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Libraries\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Data extraction and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "# Data visualization\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "\n",
    "# Satistical Analysis and Preprocessing\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Modelers\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier,\n",
    "                              VotingClassifier, GradientBoostingClassifier, AdaBoostClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('./pipelines/*')\n",
    "for f in files:\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "df_train = pd.read_csv(\"./dataset/train.csv\")\n",
    "\n",
    "df = df_train.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df.TARGET==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline elements for sckit-learn\n",
    "pipeline = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to split our data into two parts, train set and test test. But we need to make sure that the ratio of TARGET value is preserved. So we evaluate the distribution of TARGET feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Columns: 371 entries, ID to TARGET\n",
      "dtypes: float64(111), int64(260)\n",
      "memory usage: 5.7 MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    1924\n",
       "1      76\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.info())\n",
    "df.TARGET.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that data is imbalanced so we use stratified sampling to preserve the each catagory ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n",
    "\n",
    "for train_index, test_index in split.split(df, df.TARGET):\n",
    "    tr = df.loc[train_index]             #Stratified train set\n",
    "    ts = df.loc[test_index]              #Stratified test set    \n",
    "tr.reset_index(inplace=True, drop=True)\n",
    "ts.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set TARGET ratio \n",
      "0    0.961875\n",
      "1    0.038125\n",
      "Name: TARGET, dtype: float64 \n",
      "\n",
      "Test set TARGET ratio \n",
      "0    0.9625\n",
      "1    0.0375\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set TARGET ratio \\n{tr.TARGET.value_counts(normalize=True)} \\n')\n",
    "print(f'Test set TARGET ratio \\n{ts.TARGET.value_counts(normalize=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 dropping ID \n",
    "ID can be considered as a feature, so we drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_id(X):\n",
    "    return X.drop(columns=[\"ID\"])\n",
    "\n",
    "# Pipeline #0: drop_id\n",
    "fs_pl0 = ('drop_id', FunctionTransformer(drop_id, validate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature types\n",
    "\n",
    "Now that test set has been put aside , we can work on our train set with ease of mind. Let's have a look at the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Catagorical features\n",
    "First let's see how many of our features are of caragorical data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.select_dtypes(['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of them are of catagorical data type, which is not normal, it probably means all of them have been mapped to numbers already. <br>\n",
    "#### 2.1.2 Numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'var3', 'var15', 'imp_ent_var16_ult1', 'imp_op_var39_comer_ult1',\n",
       "       'imp_op_var39_comer_ult3', 'imp_op_var40_comer_ult1',\n",
       "       'imp_op_var40_comer_ult3', 'imp_op_var40_efect_ult1',\n",
       "       'imp_op_var40_efect_ult3',\n",
       "       ...\n",
       "       'saldo_medio_var33_hace2', 'saldo_medio_var33_hace3',\n",
       "       'saldo_medio_var33_ult1', 'saldo_medio_var33_ult3',\n",
       "       'saldo_medio_var44_hace2', 'saldo_medio_var44_hace3',\n",
       "       'saldo_medio_var44_ult1', 'saldo_medio_var44_ult3', 'var38', 'TARGET'],\n",
       "      dtype='object', length=371)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.select_dtypes(exclude=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Missing Values\n",
    "\n",
    "For various reasons, many real world datasets contain missing values, often encoded as blanks, NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators which assume that all values in an array are numerical, and that all have and hold meaning. A basic strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. However, this comes at the price of losing data which may be valuable (even though incomplete). A better strategy is to impute the missing values, i.e., to infer them from the known part of the data.<br>\n",
    "\n",
    "#### 2.2.1 Univariate feature imputation\n",
    "The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr.describe().transpose().sort_values(by='count').to_excel(\"tr.describe.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing value! But it's always helpful to have a missing value handler. So when the predictor goes live, it would be capable of handling any missing value.<br>\n",
    "We add a simple imputer with median as the filling strategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline #1: imputer\n",
    "fs_pl1 = ('imputer', SimpleImputer(strategy=\"median\", fill_value='missing'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Single unique value\n",
    "\n",
    "Features with only one unique value will not have any effect on predictor, so we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unique(X):  \n",
    "    try:\n",
    "        with open(\"./pipelines/drop_unique.txt\", \"rb\") as fp:\n",
    "            feats = pickle.load(fp)\n",
    "    \n",
    "    except IOError:\n",
    "        col = X.nunique()\n",
    "        feats = list(col[col>1].index)\n",
    "        with open(\"./pipelines/drop_unique.txt\", \"wb\") as fp:\n",
    "            pickle.dump(feats, fp)\n",
    "    \n",
    "    return X[feats]\n",
    "\n",
    "# Pipeline #2: drop_unique\n",
    "fs_pl2 = ('drop_unique', FunctionTransformer(drop_unique, validate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Identical Features\n",
    "\n",
    "Some features are repeated more than one and they are identical, so we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_identical(X):\n",
    "    \n",
    "    def getDuplicateColumns(df):\n",
    "        feats = list(combinations(X.columns.tolist(),2))\n",
    "        return [f[1] for f in feats if np.array_equal(X[f[0]],X[f[1]])]\n",
    "    \n",
    "    try:\n",
    "        with open(\"./pipelines/drop_identical.txt\", \"rb\") as fp:\n",
    "            feats = pickle.load(fp)\n",
    "    \n",
    "    except IOError:\n",
    "        \n",
    "        feats = getDuplicateColumns(X)\n",
    "        \n",
    "        with open(\"./pipelines/drop_identical.txt\", \"wb\") as fp:\n",
    "            pickle.dump(feats, fp)\n",
    "\n",
    "    return X.drop(columns=feats)\n",
    "\n",
    "# Pipeline #3: drop_identical\n",
    "fs_pl3 = ('drop_identical', FunctionTransformer(drop_identical, validate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Feature Importance\n",
    "\n",
    "Random forest consists of a number of decision trees. Every node in the decision trees is a condition on a single feature, designed to split the dataset into two so that similar response values end up in the same set. The measure based on which the (locally) optimal condition is chosen is called impurity. For classification, it is typically either Gini impurity or information gain/entropy and for regression trees it is variance. Thus when training a tree, it can be computed how much each feature decreases the weighted impurity in a tree. For a forest, the impurity decrease from each feature can be averaged and the features are ranked according to this measure.<br>\n",
    "There are a few things to keep in mind when using the impurity based ranking. Firstly, feature selection based on impurity reduction is biased towards preferring variables with more categories. Secondly, when the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others. But once one of them is used, the importance of others is significantly reduced since effectively the impurity they can remove is already removed by the first feature. As a consequence, they will have a lower reported importance. This is not an issue when we want to use feature selection to reduce overfitting, since it makes sense to remove features that are mostly duplicated by other features. But when interpreting the data, it can lead to the incorrect conclusion that one of the variables is a strong predictor while the others in the same group are unimportant, while actually they are very close in terms of their relationship with the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_unimportant(x):\n",
    "    \n",
    "    try:\n",
    "        with open(\"./pipelines/drop_unimportant.txt\", \"rb\") as fp:\n",
    "            feats = pickle.load(fp)\n",
    "    \n",
    "    except IOError:\n",
    "    \n",
    "        # Threshold of score\n",
    "        Score = 0\n",
    "        feats = []\n",
    "\n",
    "        X=(x-x.min())/(x.max()-x.min())\n",
    "\n",
    "        rnd_clf = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "        rnd_clf.fit(X.drop(columns=[\"TARGET\"]), X[\"TARGET\"])\n",
    "        for name, score in sorted(zip(X.drop(columns=[\"TARGET\"]).columns, rnd_clf.feature_importances_), \n",
    "                                  key=lambda x:x[1], reverse=True):\n",
    "            if score>Score: feats.append(name)\n",
    "\n",
    "        feats.append(\"TARGET\")\n",
    "        \n",
    "        with open(\"./pipelines/drop_unimportant.txt\", \"wb\") as fp:\n",
    "            pickle.dump(feats, fp)\n",
    "    \n",
    "    return x[feats]\n",
    "\n",
    "# Pipeline #3: drop_identical\n",
    "fs_pl4 = ('drop_unimportant', FunctionTransformer(drop_unimportant, validate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FS_pipe = Pipeline([fs_pl0, fs_pl2, fs_pl3, fs_pl4])\n",
    "\n",
    "df_FS_pipe = FS_pipe.fit_transform(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Summary of Feature Selection\n",
    "\n",
    "In order to create the final pipeline we hardcode all the selected features into one pipeline to be used for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['var38', 'var15', 'saldo_medio_var5_ult3', 'saldo_medio_var5_hace3', 'num_var45_hace3', 'num_var45_ult3', 'num_var22_ult3', 'saldo_var42', 'saldo_var30', 'saldo_var5', 'saldo_medio_var5_hace2', 'saldo_medio_var5_ult1', 'num_var22_ult1', 'num_var22_hace3', 'num_var45_hace2', 'num_meses_var39_vig_ult3', 'imp_op_var41_comer_ult1', 'var36', 'imp_op_var41_ult1', 'num_med_var45_ult3', 'num_meses_var5_ult3', 'num_var45_ult1', 'imp_op_var39_ult1', 'ind_var7_recib_ult1', 'num_var4', 'imp_ent_var16_ult1', 'num_var22_hace2', 'imp_op_var41_efect_ult3', 'num_var35', 'imp_op_var41_efect_ult1', 'imp_op_var41_comer_ult3', 'imp_op_var39_efect_ult3', 'imp_op_var39_comer_ult1', 'num_ent_var16_ult1', 'num_var43_emit_ult1', 'num_op_var39_ult3', 'num_var7_recib_ult1', 'imp_trans_var37_ult1', 'imp_op_var39_efect_ult1', 'imp_op_var39_comer_ult3', 'num_op_var41_ult3', 'num_med_var22_ult3', 'imp_var43_emit_ult1', 'num_var43_recib_ult1', 'imp_var7_recib_ult1', 'var3', 'num_var41_0', 'ind_var41_0', 'num_op_var41_comer_ult1', 'num_op_var39_comer_ult3', 'num_op_var39_ult1', 'num_var30', 'num_op_var41_efect_ult3', 'ind_var43_recib_ult1', 'num_op_var41_comer_ult3', 'num_op_var41_hace2', 'ind_var39_0', 'num_op_var41_efect_ult1', 'num_op_var40_ult3', 'num_var26_0', 'num_op_var41_ult1', 'num_var37_0', 'num_op_var39_efect_ult3', 'num_var39_0', 'saldo_medio_var8_ult3', 'num_op_var39_comer_ult1', 'num_trasp_var11_ult1', 'saldo_var37', 'num_op_var39_hace2', 'num_op_var40_ult1', 'ind_var30', 'num_var42', 'imp_op_var40_comer_ult1', 'ind_var43_emit_ult1', 'num_var25_0', 'num_var30_0', 'imp_op_var40_ult1', 'num_op_var40_comer_ult3', 'ind_var26_cte', 'num_var8_0', 'ind_var5', 'num_var5', 'ind_var5_0', 'saldo_var1', 'ind_var10cte_ult1', 'num_op_var39_efect_ult1', 'ind_var9_cte_ult1', 'num_var37_med_ult2', 'num_var5_0', 'saldo_var25', 'saldo_var8', 'ind_var37_cte', 'ind_var37_0', 'imp_op_var40_efect_ult1', 'num_op_var40_efect_ult1', 'num_op_var40_comer_ult1', 'ind_var9_ult1', 'num_var12_0', 'num_meses_var8_ult3', 'saldo_var26', 'saldo_medio_var8_ult1', 'ind_var10_ult1', 'ind_var26_0', 'num_var13_0', 'saldo_medio_var12_ult1', 'saldo_medio_var8_hace3', 'num_var1', 'num_var42_0', 'saldo_var12', 'saldo_medio_var8_hace2', 'ind_var25_0', 'ind_var30_0', 'ind_var13_corto_0', 'ind_var12_0', 'num_var8', 'num_var1_0', 'ind_var8_0', 'num_var24_0', 'var21', 'ind_var12', 'ind_var13_corto', 'num_op_var41_hace3', 'num_var12', 'saldo_medio_var13_corto_ult1', 'saldo_medio_var12_hace2', 'ind_var1', 'saldo_medio_var13_corto_ult3', 'ind_var8', 'num_var13_corto_0', 'num_meses_var12_ult3', 'ind_var25_cte', 'saldo_medio_var12_ult3', 'num_var13_corto', 'saldo_var14', 'imp_op_var40_efect_ult3', 'ind_var24_0', 'num_var20', 'ind_var17_0', 'num_op_var39_hace3', 'saldo_medio_var17_hace2', 'saldo_medio_var13_largo_ult1', 'ind_var13', 'ind_var13_0', 'ind_var1_0', 'saldo_var13', 'num_meses_var13_corto_ult3', 'saldo_medio_var13_corto_hace3', 'imp_aport_var13_hace3', 'ind_var20', 'ind_var31_0', 'num_var17_0', 'saldo_var13_corto', 'saldo_medio_var12_hace3', 'num_var13', 'num_var20_0', 'num_var31_0', 'saldo_var20', 'imp_op_var40_comer_ult3', 'ind_var20_0', 'saldo_medio_var13_corto_hace2', 'num_var14_0', 'num_aport_var13_hace3', 'ind_var14_0', 'ind_var24', 'saldo_var13_largo', 'num_var24', 'saldo_medio_var13_largo_hace2', 'num_var17', 'TARGET']\n"
     ]
    }
   ],
   "source": [
    "print(df_FS_pipe.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class selected_features(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        feats = ['var38', 'var15', 'saldo_medio_var5_ult3', 'num_var45_ult3', 'saldo_medio_var5_hace3', 'num_var45_hace3', \n",
    "                 'saldo_medio_var5_hace2', 'num_var22_ult3', 'num_var45_hace2', 'saldo_medio_var5_ult1', 'saldo_var30', \n",
    "                 'saldo_var42', 'saldo_var5', 'num_var45_ult1', 'num_var22_hace2', 'num_med_var45_ult3', 'num_var22_hace3',\n",
    "                 'num_var22_ult1', 'var36', 'num_meses_var39_vig_ult3', 'num_meses_var5_ult3', 'num_med_var22_ult3', \n",
    "                 'num_var35', 'imp_op_var39_ult1', 'imp_op_var41_comer_ult3', 'imp_op_var41_ult1', 'imp_op_var39_comer_ult3',\n",
    "                 'num_var4', 'imp_op_var41_efect_ult3', 'imp_trans_var37_ult1', 'imp_op_var39_efect_ult3',\n",
    "                 'imp_op_var39_comer_ult1', 'num_op_var39_ult3', 'num_op_var41_ult3', 'imp_op_var41_comer_ult1', \n",
    "                 'ind_var5', 'imp_op_var41_efect_ult1', 'var3', 'imp_op_var39_efect_ult1', 'num_op_var39_comer_ult3', \n",
    "                 'imp_ent_var16_ult1', 'saldo_var37', 'num_op_var39_ult1', 'num_op_var41_hace2', 'num_var30', \n",
    "                 'num_op_var41_ult1', 'imp_var43_emit_ult1', 'num_op_var41_comer_ult3', 'num_op_var41_efect_ult3', \n",
    "                 'num_op_var39_efect_ult3', 'num_op_var39_hace2', 'ind_var30', 'num_op_var41_comer_ult1', 'num_var5',\n",
    "                 'num_op_var39_comer_ult1', 'num_var43_recib_ult1', 'num_var42', 'num_var39_0', 'num_ent_var16_ult1', \n",
    "                 'num_var41_0', 'saldo_medio_var8_ult1', 'saldo_medio_var8_ult3', 'num_var43_emit_ult1', 'num_var37_0', \n",
    "                 'num_op_var39_efect_ult1', 'saldo_var8', 'num_op_var41_efect_ult1', 'num_var37_med_ult2', \n",
    "                 'ind_var43_recib_ult1', 'num_var5_0', 'num_var30_0', 'num_var42_0', 'num_var8_0', 'ind_var8_0',\n",
    "                 'saldo_var26', 'ind_var41_0', 'ind_var39_0', 'saldo_medio_var8_hace2', 'saldo_var25',\n",
    "                 'ind_var43_emit_ult1', 'ind_var10cte_ult1', 'ind_var37_cte', 'saldo_medio_var12_ult1', \n",
    "                 'ind_var9_cte_ult1', 'ind_var9_ult1', 'saldo_medio_var13_corto_ult3', 'num_trasp_var11_ult1', \n",
    "                 'ind_var37_0', 'ind_var10_ult1', 'saldo_var12', 'ind_var5_0', 'num_var26_0', 'saldo_medio_var12_ult3', \n",
    "                 'saldo_medio_var12_hace2', 'ind_var12_0', 'num_op_var39_hace3', 'saldo_medio_var13_corto_hace2',\n",
    "                 'num_var12_0', 'saldo_medio_var8_hace3', 'saldo_var13_corto', 'num_var25_0', 'num_meses_var8_ult3', \n",
    "                 'imp_op_var40_efect_ult1', 'num_op_var41_hace3', 'num_var24_0', 'saldo_var13', \n",
    "                 'saldo_medio_var13_corto_ult1', 'saldo_var24', 'num_sal_var16_ult1', 'imp_sal_var16_ult1', 'ind_var26_cte',\n",
    "                 'imp_var7_recib_ult1', 'var21', 'imp_op_var40_efect_ult3', 'num_var14_0', 'ind_var25_cte', \n",
    "                 'num_meses_var12_ult3', 'num_var13', 'ind_var24_0', 'ind_var8', 'ind_var26_0', \n",
    "                 'num_meses_var13_corto_ult3', 'saldo_medio_var12_hace3', 'num_var1_0', 'ind_var25_0', 'saldo_var40', \n",
    "                 'num_var40_0', 'ind_var13_0', 'num_var7_recib_ult1', 'num_var12', 'ind_var14_0', 'num_op_var40_efect_ult3',\n",
    "                 'num_var13_0', 'num_var8', 'saldo_var1', 'ind_var30_0', 'ind_var1_0', 'num_op_var40_efect_ult1',\n",
    "                 'ind_var7_recib_ult1', 'ind_var13', 'saldo_var14', 'num_op_var40_ult1', 'num_op_var40_comer_ult3',\n",
    "                 'ind_var12', 'ind_var40_0', 'ind_var13_corto_0', 'saldo_var32', 'num_var24', 'num_op_var40_comer_ult1', \n",
    "                 'imp_op_var40_comer_ult3', 'imp_op_var40_comer_ult1', 'ind_var24', 'num_op_var40_ult3', 'num_var13_corto',\n",
    "                 'num_var13_corto_0', 'ind_var14', 'imp_op_var40_ult1', 'ind_var13_corto', 'saldo_medio_var13_corto_hace3',\n",
    "                 'num_var32_0', 'imp_aport_var13_hace3', 'ind_var32_cte', 'imp_aport_var13_ult1', 'saldo_var44',\n",
    "                 'saldo_var13_largo', 'num_aport_var13_hace3', 'num_aport_var17_ult1', 'ind_var1', 'ind_var13_largo', \n",
    "                 'imp_reemb_var13_ult1', 'saldo_medio_var17_ult3', 'num_reemb_var17_ult1', 'ind_var40', 'num_var14', \n",
    "                 'saldo_medio_var44_ult1', 'num_var1', 'imp_compra_var44_ult1', 'ind_var19', 'saldo_var31', 'ind_var31_0',\n",
    "                 'num_op_var40_hace2', 'imp_reemb_var17_ult1', 'ind_var13_largo_0', 'imp_aport_var17_ult1', 'num_var40',\n",
    "                 'ind_var32_0', 'num_var13_largo_0', 'delta_num_aport_var13_1y3', 'num_var31', 'delta_imp_aport_var13_1y3',\n",
    "                 'num_var31_0', 'num_reemb_var13_ult1', 'num_aport_var13_ult1', 'num_var13_largo', 'num_var17_0', \n",
    "                 'saldo_medio_var17_ult1', 'ind_var31', 'delta_num_compra_var44_1y3', 'ind_var20_0', 'ind_var44_0', \n",
    "                 'saldo_medio_var44_ult3', 'num_meses_var13_largo_ult3', 'num_meses_var44_ult3', 'num_var20_0', \n",
    "                 'saldo_medio_var13_largo_ult1', 'delta_imp_reemb_var13_1y3', 'ind_var44', 'num_compra_var44_ult1', \n",
    "                 'ind_var17_0', 'num_var33_0', 'saldo_medio_var13_largo_hace2', 'delta_num_venta_var44_1y3', 'num_var44_0',\n",
    "                 'num_var17', 'delta_imp_compra_var44_1y3', 'num_meses_var17_ult3', 'saldo_medio_var13_largo_ult3', \n",
    "                 'saldo_var17', 'num_var44', 'saldo_medio_var17_hace2', 'delta_imp_venta_var44_1y3', \n",
    "                 'delta_imp_reemb_var17_1y3', 'saldo_medio_var33_ult1', 'saldo_medio_var44_hace2', 'saldo_var20',\n",
    "                 'imp_venta_var44_ult1', 'saldo_medio_var13_largo_hace3', 'num_meses_var33_ult3', 'imp_aport_var33_ult1',\n",
    "                 'ind_var33_0', 'ind_var20', 'delta_imp_trasp_var33_out_1y3', 'imp_venta_var44_hace3', 'ind_var17', \n",
    "                 'delta_imp_aport_var17_1y3', 'saldo_medio_var33_ult3', 'delta_num_aport_var17_1y3', 'num_var33', \n",
    "                 'num_aport_var33_hace3', 'saldo_var33', 'num_venta_var44_ult1', 'num_trasp_var33_in_hace3', \n",
    "                 'saldo_medio_var33_hace2', 'imp_aport_var17_hace3', 'saldo_medio_var33_hace3', 'ind_var6_0', \n",
    "                 'saldo_medio_var44_hace3', 'num_var20', 'imp_trasp_var33_in_ult1', 'imp_trasp_var17_out_ult1', \n",
    "                 'imp_trasp_var17_in_ult1', 'delta_imp_aport_var33_1y3', 'num_compra_var44_hace3']\n",
    "        \n",
    "        return X[feats]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "\n",
    "# Just this pipeline to be used for the next steps\n",
    "fs_pl = ('selected_features', selected_features())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "\n",
    "A critical part of the success of a Machine Learning project is coming up with a\n",
    "good set of features to train on. This process, called feature engineering, which involves combining existing features to produce a more useful one using dimensionality reduction algorithms.\n",
    "\n",
    "<br>\n",
    "Following methods are used for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DimReducer = [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.95)\n",
    "DimReducer.append(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpca = KernelPCA(n_components = 5, kernel=\"rbf\", gamma=0.04)\n",
    "DimReducer.append(kpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Locally Linear Embedding (LLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lle = LocallyLinearEmbedding(n_components=8, n_neighbors=40)\n",
    "DimReducer.append(lle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Isomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = Isomap(n_components=4, n_neighbors=40)\n",
    "DimReducer.append(isomap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_pl = ('dim_reducer', DimReducer[0])\n",
    "param_grid = {'dim_reducer': DimReducer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modeler = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=random_state)\n",
    "Modeler.append(log_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf = RandomForestClassifier(random_state=random_state)\n",
    "Modeler.append(rnd_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(gamma=\"auto\", random_state=random_state)\n",
    "Modeler.append(svm_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=random_state), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=random_state)\n",
    "Modeler.append(bag_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 AdaBoost Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.5, random_state=random_state)\n",
    "Modeler.append(ada_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 XGBoost Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgboost.XGBClassifier(random_state=random_state)\n",
    "Modeler.append(xgb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pl = ('modeler', Modeler[0])\n",
    "param_grid.update({'modeler': Modeler})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[fs_pl])\n",
    "TR_ = pipe.fit_transform(tr)\n",
    "\n",
    "TR = pd.concat([TR_, tr.TARGET], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[fs_pl, fe_pl, ml_pl])\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=3, n_jobs=1, param_grid=param_grid, verbose=1, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('selected_features',\n",
       "                                        selected_features()),\n",
       "                                       ('dim_reducer', None),\n",
       "                                       ('modeler',\n",
       "                                        LogisticRegression(C=1.0,\n",
       "                                                           class_weight=None,\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=100,\n",
       "                                                           multi_class='warn',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=42,\n",
       "                                                           solver='...\n",
       "                                                   colsample_bytree=1, gamma=0,\n",
       "                                                   learning_rate=0.1,\n",
       "                                                   max_delta_step=0,\n",
       "                                                   max_depth=3,\n",
       "                                                   min_child_weight=1,\n",
       "                                                   missing=None,\n",
       "                                                   n_estimators=100, n_jobs=1,\n",
       "                                                   nthread=None,\n",
       "                                                   objective='binary:logistic',\n",
       "                                                   random_state=42, reg_alpha=0,\n",
       "                                                   reg_lambda=1,\n",
       "                                                   scale_pos_weight=1,\n",
       "                                                   seed=None, silent=None,\n",
       "                                                   subsample=1, verbosity=1)]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(TR.drop(columns=[\"TARGET\"]), TR.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>param_dim_reducer</th>\n",
       "      <th>param_modeler</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.403348</td>\n",
       "      <td>None</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.811728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.782136</td>\n",
       "      <td>None</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.754880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.065737</td>\n",
       "      <td>None</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.708118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034885</td>\n",
       "      <td>None</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.677273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.913292</td>\n",
       "      <td>LocallyLinearEmbedding(eigen_solver='auto', he...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.633136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.758217</td>\n",
       "      <td>LocallyLinearEmbedding(eigen_solver='auto', he...</td>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.618376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025131</td>\n",
       "      <td>None</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.609302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.101286</td>\n",
       "      <td>LocallyLinearEmbedding(eigen_solver='auto', he...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.553954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.565078</td>\n",
       "      <td>Isomap(eigen_solver='auto', max_iter=None, n_c...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.553540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.366149</td>\n",
       "      <td>Isomap(eigen_solver='auto', max_iter=None, n_c...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.547556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.779205</td>\n",
       "      <td>LocallyLinearEmbedding(eigen_solver='auto', he...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.531303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.313582</td>\n",
       "      <td>Isomap(eigen_solver='auto', max_iter=None, n_c...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.524313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.187612</td>\n",
       "      <td>LocallyLinearEmbedding(eigen_solver='auto', he...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.524172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.314298</td>\n",
       "      <td>KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.520921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.312987</td>\n",
       "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.520512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.101185</td>\n",
       "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.517932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.875274</td>\n",
       "      <td>KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.516333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.316713</td>\n",
       "      <td>Isomap(eigen_solver='auto', max_iter=None, n_c...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.515653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.668707</td>\n",
       "      <td>Isomap(eigen_solver='auto', max_iter=None, n_c...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.513118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.310992</td>\n",
       "      <td>KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.504372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.853429</td>\n",
       "      <td>LocallyLinearEmbedding(eigen_solver='auto', he...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.502791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.408538</td>\n",
       "      <td>KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...</td>\n",
       "      <td>XGBClassifier(base_score=0.5, booster='gbtree'...</td>\n",
       "      <td>0.502491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.707957</td>\n",
       "      <td>KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.499047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.391939</td>\n",
       "      <td>Isomap(eigen_solver='auto', max_iter=None, n_c...</td>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.494965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.089115</td>\n",
       "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.493725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.313803</td>\n",
       "      <td>KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...</td>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.492951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.430209</td>\n",
       "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
       "      <td>(DecisionTreeClassifier(class_weight=None, cri...</td>\n",
       "      <td>0.490836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.428708</td>\n",
       "      <td>None</td>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.470112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.067787</td>\n",
       "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
       "      <td>LogisticRegression(C=1.0, class_weight=None, d...</td>\n",
       "      <td>0.461829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.075226</td>\n",
       "      <td>PCA(copy=True, iterated_power='auto', n_compon...</td>\n",
       "      <td>SVC(C=1.0, cache_size=200, class_weight=None, ...</td>\n",
       "      <td>0.451170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time                                  param_dim_reducer  \\\n",
       "5        1.403348                                               None   \n",
       "3        1.782136                                               None   \n",
       "4        1.065737                                               None   \n",
       "1        0.034885                                               None   \n",
       "18       0.913292  LocallyLinearEmbedding(eigen_solver='auto', he...   \n",
       "20       0.758217  LocallyLinearEmbedding(eigen_solver='auto', he...   \n",
       "0        0.025131                                               None   \n",
       "22       1.101286  LocallyLinearEmbedding(eigen_solver='auto', he...   \n",
       "28       1.565078  Isomap(eigen_solver='auto', max_iter=None, n_c...   \n",
       "29       1.366149  Isomap(eigen_solver='auto', max_iter=None, n_c...   \n",
       "19       0.779205  LocallyLinearEmbedding(eigen_solver='auto', he...   \n",
       "24       1.313582  Isomap(eigen_solver='auto', max_iter=None, n_c...   \n",
       "21       1.187612  LocallyLinearEmbedding(eigen_solver='auto', he...   \n",
       "13       0.314298  KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...   \n",
       "10       0.312987  PCA(copy=True, iterated_power='auto', n_compon...   \n",
       "11       0.101185  PCA(copy=True, iterated_power='auto', n_compon...   \n",
       "15       0.875274  KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...   \n",
       "25       1.316713  Isomap(eigen_solver='auto', max_iter=None, n_c...   \n",
       "27       1.668707  Isomap(eigen_solver='auto', max_iter=None, n_c...   \n",
       "12       0.310992  KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...   \n",
       "23       0.853429  LocallyLinearEmbedding(eigen_solver='auto', he...   \n",
       "17       0.408538  KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...   \n",
       "16       0.707957  KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...   \n",
       "26       1.391939  Isomap(eigen_solver='auto', max_iter=None, n_c...   \n",
       "7        0.089115  PCA(copy=True, iterated_power='auto', n_compon...   \n",
       "14       0.313803  KernelPCA(alpha=1.0, coef0=1, copy_X=True, deg...   \n",
       "9        0.430209  PCA(copy=True, iterated_power='auto', n_compon...   \n",
       "2        0.428708                                               None   \n",
       "6        0.067787  PCA(copy=True, iterated_power='auto', n_compon...   \n",
       "8        0.075226  PCA(copy=True, iterated_power='auto', n_compon...   \n",
       "\n",
       "                                        param_modeler  mean_test_score  \n",
       "5   XGBClassifier(base_score=0.5, booster='gbtree'...         0.811728  \n",
       "3   (DecisionTreeClassifier(class_weight=None, cri...         0.754880  \n",
       "4   (DecisionTreeClassifier(class_weight=None, cri...         0.708118  \n",
       "1   (DecisionTreeClassifier(class_weight=None, cri...         0.677273  \n",
       "18  LogisticRegression(C=1.0, class_weight=None, d...         0.633136  \n",
       "20  SVC(C=1.0, cache_size=200, class_weight=None, ...         0.618376  \n",
       "0   LogisticRegression(C=1.0, class_weight=None, d...         0.609302  \n",
       "22  (DecisionTreeClassifier(class_weight=None, cri...         0.553954  \n",
       "28  (DecisionTreeClassifier(class_weight=None, cri...         0.553540  \n",
       "29  XGBClassifier(base_score=0.5, booster='gbtree'...         0.547556  \n",
       "19  (DecisionTreeClassifier(class_weight=None, cri...         0.531303  \n",
       "24  LogisticRegression(C=1.0, class_weight=None, d...         0.524313  \n",
       "21  (DecisionTreeClassifier(class_weight=None, cri...         0.524172  \n",
       "13  (DecisionTreeClassifier(class_weight=None, cri...         0.520921  \n",
       "10  (DecisionTreeClassifier(class_weight=None, cri...         0.520512  \n",
       "11  XGBClassifier(base_score=0.5, booster='gbtree'...         0.517932  \n",
       "15  (DecisionTreeClassifier(class_weight=None, cri...         0.516333  \n",
       "25  (DecisionTreeClassifier(class_weight=None, cri...         0.515653  \n",
       "27  (DecisionTreeClassifier(class_weight=None, cri...         0.513118  \n",
       "12  LogisticRegression(C=1.0, class_weight=None, d...         0.504372  \n",
       "23  XGBClassifier(base_score=0.5, booster='gbtree'...         0.502791  \n",
       "17  XGBClassifier(base_score=0.5, booster='gbtree'...         0.502491  \n",
       "16  (DecisionTreeClassifier(class_weight=None, cri...         0.499047  \n",
       "26  SVC(C=1.0, cache_size=200, class_weight=None, ...         0.494965  \n",
       "7   (DecisionTreeClassifier(class_weight=None, cri...         0.493725  \n",
       "14  SVC(C=1.0, cache_size=200, class_weight=None, ...         0.492951  \n",
       "9   (DecisionTreeClassifier(class_weight=None, cri...         0.490836  \n",
       "2   SVC(C=1.0, cache_size=200, class_weight=None, ...         0.470112  \n",
       "6   LogisticRegression(C=1.0, class_weight=None, d...         0.461829  \n",
       "8   SVC(C=1.0, cache_size=200, class_weight=None, ...         0.451170  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_).sort_values('rank_test_score')[['mean_fit_time', 'param_dim_reducer',\n",
    "                                                                'param_modeler', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Selection and Hyperparameter Tuning\n",
    "\n",
    "It can be seen that xgboost outperforms all algorithms without any diemsionality reduction implementation.\n",
    "So we use this set to tune their hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf = xgboost.XGBClassifier(random_state=random_state)\n",
    "ml_pl = ('xgboost', bag_clf)\n",
    "\n",
    "param_grid = {'xgboost__objective':['binary:logistic', 'reg:squarederror'],\n",
    "              'xgboost__learning_rate': [0.04], #so called `eta` value\n",
    "              'xgboost__max_depth': [6, 7, 8],\n",
    "              'xgboost__min_child_weight': [2, 11],\n",
    "              'xgboost__subsample': [0.8],\n",
    "              'xgboost__colsample_bytree': [0.8],\n",
    "              'xgboost__n_estimators': [90, 100], #number of trees, change it to 1000 for better results\n",
    "             }\n",
    "\n",
    "pipe_final = Pipeline(steps=[fs_pl, ml_pl])\n",
    "grid = GridSearchCV(pipe_final, cv=3, n_jobs=1, param_grid=param_grid, verbose=1, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('selected_features',\n",
       "                                        selected_features()),\n",
       "                                       ('xgboost',\n",
       "                                        XGBClassifier(base_score=0.5,\n",
       "                                                      booster='gbtree',\n",
       "                                                      colsample_bylevel=1,\n",
       "                                                      colsample_bynode=1,\n",
       "                                                      colsample_bytree=1,\n",
       "                                                      gamma=0,\n",
       "                                                      learning_rate=0.1,\n",
       "                                                      max_delta_step=0,\n",
       "                                                      max_depth=3,\n",
       "                                                      min_child_weight=1,\n",
       "                                                      missing=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n...\n",
       "             param_grid={'xgboost__colsample_bytree': [0.8],\n",
       "                         'xgboost__learning_rate': [0.04],\n",
       "                         'xgboost__max_depth': [6, 7, 8],\n",
       "                         'xgboost__min_child_weight': [2, 11],\n",
       "                         'xgboost__n_estimators': [90, 100],\n",
       "                         'xgboost__objective': ['binary:logistic',\n",
       "                                                'reg:squarederror'],\n",
       "                         'xgboost__subsample': [0.8]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(tr.drop(columns=[\"TARGET\"]), tr.TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_xgboost__colsample_bytree</th>\n",
       "      <th>param_xgboost__learning_rate</th>\n",
       "      <th>param_xgboost__max_depth</th>\n",
       "      <th>param_xgboost__min_child_weight</th>\n",
       "      <th>param_xgboost__n_estimators</th>\n",
       "      <th>param_xgboost__objective</th>\n",
       "      <th>param_xgboost__subsample</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.867007</td>\n",
       "      <td>0.009574</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>2.337183e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>0.761014</td>\n",
       "      <td>0.866910</td>\n",
       "      <td>0.821511</td>\n",
       "      <td>0.044515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.863394</td>\n",
       "      <td>0.016686</td>\n",
       "      <td>0.010249</td>\n",
       "      <td>1.167468e-03</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>0.761014</td>\n",
       "      <td>0.866910</td>\n",
       "      <td>0.821511</td>\n",
       "      <td>0.044515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.872971</td>\n",
       "      <td>0.014221</td>\n",
       "      <td>0.009423</td>\n",
       "      <td>2.306083e-06</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.836582</td>\n",
       "      <td>0.761014</td>\n",
       "      <td>0.866910</td>\n",
       "      <td>0.821511</td>\n",
       "      <td>0.044515</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.153044</td>\n",
       "      <td>0.020445</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>4.675494e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.844008</td>\n",
       "      <td>0.748343</td>\n",
       "      <td>0.870809</td>\n",
       "      <td>0.821068</td>\n",
       "      <td>0.052552</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.190894</td>\n",
       "      <td>0.013259</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>4.053961e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.849253</td>\n",
       "      <td>0.750585</td>\n",
       "      <td>0.861062</td>\n",
       "      <td>0.820318</td>\n",
       "      <td>0.049521</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.056005</td>\n",
       "      <td>0.016231</td>\n",
       "      <td>0.010228</td>\n",
       "      <td>4.623637e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.842801</td>\n",
       "      <td>0.744981</td>\n",
       "      <td>0.872466</td>\n",
       "      <td>0.820097</td>\n",
       "      <td>0.054455</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.243977</td>\n",
       "      <td>0.025321</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>4.466043e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.845540</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.864084</td>\n",
       "      <td>0.819891</td>\n",
       "      <td>0.049974</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.120147</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>0.009926</td>\n",
       "      <td>4.182440e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.843683</td>\n",
       "      <td>0.748051</td>\n",
       "      <td>0.867203</td>\n",
       "      <td>0.819661</td>\n",
       "      <td>0.051515</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.071866</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>2.322573e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.845447</td>\n",
       "      <td>0.747856</td>\n",
       "      <td>0.863596</td>\n",
       "      <td>0.818983</td>\n",
       "      <td>0.050814</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.802207</td>\n",
       "      <td>0.015801</td>\n",
       "      <td>0.009755</td>\n",
       "      <td>8.426212e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>90</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.837464</td>\n",
       "      <td>0.753119</td>\n",
       "      <td>0.862427</td>\n",
       "      <td>0.817682</td>\n",
       "      <td>0.046756</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.791297</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.009422</td>\n",
       "      <td>4.026735e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>90</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.837464</td>\n",
       "      <td>0.753119</td>\n",
       "      <td>0.862427</td>\n",
       "      <td>0.817682</td>\n",
       "      <td>0.046756</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.798063</td>\n",
       "      <td>0.017009</td>\n",
       "      <td>0.009612</td>\n",
       "      <td>6.022131e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>90</td>\n",
       "      <td>binary:logistic</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.837464</td>\n",
       "      <td>0.753119</td>\n",
       "      <td>0.862427</td>\n",
       "      <td>0.817682</td>\n",
       "      <td>0.046756</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.162301</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.010418</td>\n",
       "      <td>4.034502e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>90</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.820477</td>\n",
       "      <td>0.747173</td>\n",
       "      <td>0.807407</td>\n",
       "      <td>0.791704</td>\n",
       "      <td>0.031922</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.199326</td>\n",
       "      <td>0.020665</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>2.329879e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.814397</td>\n",
       "      <td>0.739864</td>\n",
       "      <td>0.819883</td>\n",
       "      <td>0.791396</td>\n",
       "      <td>0.036490</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.331427</td>\n",
       "      <td>0.014285</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>3.928034e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.818760</td>\n",
       "      <td>0.730312</td>\n",
       "      <td>0.813158</td>\n",
       "      <td>0.787429</td>\n",
       "      <td>0.040434</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.295564</td>\n",
       "      <td>0.013335</td>\n",
       "      <td>0.011085</td>\n",
       "      <td>6.329788e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.818157</td>\n",
       "      <td>0.735673</td>\n",
       "      <td>0.806530</td>\n",
       "      <td>0.786806</td>\n",
       "      <td>0.036450</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.292409</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.010912</td>\n",
       "      <td>2.973602e-07</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>90</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.804883</td>\n",
       "      <td>0.719006</td>\n",
       "      <td>0.799513</td>\n",
       "      <td>0.774486</td>\n",
       "      <td>0.039273</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.388312</td>\n",
       "      <td>0.023064</td>\n",
       "      <td>0.011067</td>\n",
       "      <td>2.417485e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>90</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.784229</td>\n",
       "      <td>0.749318</td>\n",
       "      <td>0.777485</td>\n",
       "      <td>0.770353</td>\n",
       "      <td>0.015120</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.447987</td>\n",
       "      <td>0.037068</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>4.683358e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.795229</td>\n",
       "      <td>0.717934</td>\n",
       "      <td>0.795809</td>\n",
       "      <td>0.769673</td>\n",
       "      <td>0.036569</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.321177</td>\n",
       "      <td>0.001914</td>\n",
       "      <td>0.010251</td>\n",
       "      <td>2.336622e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.711209</td>\n",
       "      <td>0.802144</td>\n",
       "      <td>0.769264</td>\n",
       "      <td>0.041154</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.511651</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.011087</td>\n",
       "      <td>6.052069e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.777917</td>\n",
       "      <td>0.754971</td>\n",
       "      <td>0.766082</td>\n",
       "      <td>0.766330</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.499268</td>\n",
       "      <td>0.033703</td>\n",
       "      <td>0.011241</td>\n",
       "      <td>2.388955e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.812169</td>\n",
       "      <td>0.692008</td>\n",
       "      <td>0.792398</td>\n",
       "      <td>0.765554</td>\n",
       "      <td>0.052604</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.458414</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.011397</td>\n",
       "      <td>1.056872e-03</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.769145</td>\n",
       "      <td>0.687135</td>\n",
       "      <td>0.785819</td>\n",
       "      <td>0.747380</td>\n",
       "      <td>0.043121</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.598287</td>\n",
       "      <td>0.018263</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>4.033530e-04</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.04</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>reg:squarederror</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'xgboost__colsample_bytree': 0.8, 'xgboost__l...</td>\n",
       "      <td>0.770445</td>\n",
       "      <td>0.678655</td>\n",
       "      <td>0.757797</td>\n",
       "      <td>0.735654</td>\n",
       "      <td>0.040615</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "14       0.867007      0.009574         0.009093    2.337183e-04   \n",
       "22       0.863394      0.016686         0.010249    1.167468e-03   \n",
       "6        0.872971      0.014221         0.009423    2.306083e-06   \n",
       "2        1.153044      0.020445         0.009755    4.675494e-04   \n",
       "10       1.190894      0.013259         0.010417    4.053961e-04   \n",
       "0        1.056005      0.016231         0.010228    4.623637e-04   \n",
       "18       1.243977      0.025321         0.009755    4.466043e-04   \n",
       "16       1.120147      0.005228         0.009926    4.182440e-04   \n",
       "8        1.071866      0.007512         0.009588    2.322573e-04   \n",
       "12       0.802207      0.015801         0.009755    8.426212e-04   \n",
       "20       0.791297      0.000251         0.009422    4.026735e-04   \n",
       "4        0.798063      0.017009         0.009612    6.022131e-04   \n",
       "5        1.162301      0.012852         0.010418    4.034502e-04   \n",
       "1        1.199326      0.020665         0.010251    2.329879e-04   \n",
       "3        1.331427      0.014285         0.010922    3.928034e-04   \n",
       "7        1.295564      0.013335         0.011085    6.329788e-04   \n",
       "13       1.292409      0.025135         0.010912    2.973602e-07   \n",
       "21       1.388312      0.023064         0.011067    2.417485e-04   \n",
       "15       1.447987      0.037068         0.011244    4.683358e-04   \n",
       "9        1.321177      0.001914         0.010251    2.336622e-04   \n",
       "23       1.511651      0.002680         0.011087    6.052069e-04   \n",
       "11       1.499268      0.033703         0.011241    2.388955e-04   \n",
       "17       1.458414      0.006466         0.011397    1.056872e-03   \n",
       "19       1.598287      0.018263         0.011407    4.033530e-04   \n",
       "\n",
       "   param_xgboost__colsample_bytree param_xgboost__learning_rate  \\\n",
       "14                             0.8                         0.04   \n",
       "22                             0.8                         0.04   \n",
       "6                              0.8                         0.04   \n",
       "2                              0.8                         0.04   \n",
       "10                             0.8                         0.04   \n",
       "0                              0.8                         0.04   \n",
       "18                             0.8                         0.04   \n",
       "16                             0.8                         0.04   \n",
       "8                              0.8                         0.04   \n",
       "12                             0.8                         0.04   \n",
       "20                             0.8                         0.04   \n",
       "4                              0.8                         0.04   \n",
       "5                              0.8                         0.04   \n",
       "1                              0.8                         0.04   \n",
       "3                              0.8                         0.04   \n",
       "7                              0.8                         0.04   \n",
       "13                             0.8                         0.04   \n",
       "21                             0.8                         0.04   \n",
       "15                             0.8                         0.04   \n",
       "9                              0.8                         0.04   \n",
       "23                             0.8                         0.04   \n",
       "11                             0.8                         0.04   \n",
       "17                             0.8                         0.04   \n",
       "19                             0.8                         0.04   \n",
       "\n",
       "   param_xgboost__max_depth param_xgboost__min_child_weight  \\\n",
       "14                        7                              11   \n",
       "22                        8                              11   \n",
       "6                         6                              11   \n",
       "2                         6                               2   \n",
       "10                        7                               2   \n",
       "0                         6                               2   \n",
       "18                        8                               2   \n",
       "16                        8                               2   \n",
       "8                         7                               2   \n",
       "12                        7                              11   \n",
       "20                        8                              11   \n",
       "4                         6                              11   \n",
       "5                         6                              11   \n",
       "1                         6                               2   \n",
       "3                         6                               2   \n",
       "7                         6                              11   \n",
       "13                        7                              11   \n",
       "21                        8                              11   \n",
       "15                        7                              11   \n",
       "9                         7                               2   \n",
       "23                        8                              11   \n",
       "11                        7                               2   \n",
       "17                        8                               2   \n",
       "19                        8                               2   \n",
       "\n",
       "   param_xgboost__n_estimators param_xgboost__objective  \\\n",
       "14                         100          binary:logistic   \n",
       "22                         100          binary:logistic   \n",
       "6                          100          binary:logistic   \n",
       "2                          100          binary:logistic   \n",
       "10                         100          binary:logistic   \n",
       "0                           90          binary:logistic   \n",
       "18                         100          binary:logistic   \n",
       "16                          90          binary:logistic   \n",
       "8                           90          binary:logistic   \n",
       "12                          90          binary:logistic   \n",
       "20                          90          binary:logistic   \n",
       "4                           90          binary:logistic   \n",
       "5                           90         reg:squarederror   \n",
       "1                           90         reg:squarederror   \n",
       "3                          100         reg:squarederror   \n",
       "7                          100         reg:squarederror   \n",
       "13                          90         reg:squarederror   \n",
       "21                          90         reg:squarederror   \n",
       "15                         100         reg:squarederror   \n",
       "9                           90         reg:squarederror   \n",
       "23                         100         reg:squarederror   \n",
       "11                         100         reg:squarederror   \n",
       "17                          90         reg:squarederror   \n",
       "19                         100         reg:squarederror   \n",
       "\n",
       "   param_xgboost__subsample  \\\n",
       "14                      0.8   \n",
       "22                      0.8   \n",
       "6                       0.8   \n",
       "2                       0.8   \n",
       "10                      0.8   \n",
       "0                       0.8   \n",
       "18                      0.8   \n",
       "16                      0.8   \n",
       "8                       0.8   \n",
       "12                      0.8   \n",
       "20                      0.8   \n",
       "4                       0.8   \n",
       "5                       0.8   \n",
       "1                       0.8   \n",
       "3                       0.8   \n",
       "7                       0.8   \n",
       "13                      0.8   \n",
       "21                      0.8   \n",
       "15                      0.8   \n",
       "9                       0.8   \n",
       "23                      0.8   \n",
       "11                      0.8   \n",
       "17                      0.8   \n",
       "19                      0.8   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "14  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.836582   \n",
       "22  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.836582   \n",
       "6   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.836582   \n",
       "2   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.844008   \n",
       "10  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.849253   \n",
       "0   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.842801   \n",
       "18  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.845540   \n",
       "16  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.843683   \n",
       "8   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.845447   \n",
       "12  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.837464   \n",
       "20  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.837464   \n",
       "4   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.837464   \n",
       "5   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.820477   \n",
       "1   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.814397   \n",
       "3   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.818760   \n",
       "7   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.818157   \n",
       "13  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.804883   \n",
       "21  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.784229   \n",
       "15  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.795229   \n",
       "9   {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.794393   \n",
       "23  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.777917   \n",
       "11  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.812169   \n",
       "17  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.769145   \n",
       "19  {'xgboost__colsample_bytree': 0.8, 'xgboost__l...           0.770445   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "14           0.761014           0.866910         0.821511        0.044515   \n",
       "22           0.761014           0.866910         0.821511        0.044515   \n",
       "6            0.761014           0.866910         0.821511        0.044515   \n",
       "2            0.748343           0.870809         0.821068        0.052552   \n",
       "10           0.750585           0.861062         0.820318        0.049521   \n",
       "0            0.744981           0.872466         0.820097        0.054455   \n",
       "18           0.750000           0.864084         0.819891        0.049974   \n",
       "16           0.748051           0.867203         0.819661        0.051515   \n",
       "8            0.747856           0.863596         0.818983        0.050814   \n",
       "12           0.753119           0.862427         0.817682        0.046756   \n",
       "20           0.753119           0.862427         0.817682        0.046756   \n",
       "4            0.753119           0.862427         0.817682        0.046756   \n",
       "5            0.747173           0.807407         0.791704        0.031922   \n",
       "1            0.739864           0.819883         0.791396        0.036490   \n",
       "3            0.730312           0.813158         0.787429        0.040434   \n",
       "7            0.735673           0.806530         0.786806        0.036450   \n",
       "13           0.719006           0.799513         0.774486        0.039273   \n",
       "21           0.749318           0.777485         0.770353        0.015120   \n",
       "15           0.717934           0.795809         0.769673        0.036569   \n",
       "9            0.711209           0.802144         0.769264        0.041154   \n",
       "23           0.754971           0.766082         0.766330        0.009371   \n",
       "11           0.692008           0.792398         0.765554        0.052604   \n",
       "17           0.687135           0.785819         0.747380        0.043121   \n",
       "19           0.678655           0.757797         0.735654        0.040615   \n",
       "\n",
       "    rank_test_score  \n",
       "14                1  \n",
       "22                1  \n",
       "6                 1  \n",
       "2                 4  \n",
       "10                5  \n",
       "0                 6  \n",
       "18                7  \n",
       "16                8  \n",
       "8                 9  \n",
       "12               10  \n",
       "20               10  \n",
       "4                10  \n",
       "5                13  \n",
       "1                14  \n",
       "3                15  \n",
       "7                16  \n",
       "13               17  \n",
       "21               18  \n",
       "15               19  \n",
       "9                20  \n",
       "23               21  \n",
       "11               22  \n",
       "17               23  \n",
       "19               24  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid.cv_results_).sort_values('rank_test_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best parameters from grid and train XGBooster on them using the whole train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('selected_features',\n",
       "                                        selected_features()),\n",
       "                                       ('xgboost',\n",
       "                                        XGBClassifier(base_score=0.5,\n",
       "                                                      booster='gbtree',\n",
       "                                                      colsample_bylevel=1,\n",
       "                                                      colsample_bynode=1,\n",
       "                                                      colsample_bytree=1,\n",
       "                                                      gamma=0,\n",
       "                                                      learning_rate=0.1,\n",
       "                                                      max_delta_step=0,\n",
       "                                                      max_depth=3,\n",
       "                                                      min_child_weight=1,\n",
       "                                                      missing=None,\n",
       "                                                      n_estimators=100,\n",
       "                                                      n...\n",
       "                                verbose=False),\n",
       "             iid='warn', n_jobs=1,\n",
       "             param_grid={'xgboost__colsample_bytree': [0.8],\n",
       "                         'xgboost__learning_rate': [0.04],\n",
       "                         'xgboost__max_depth': [6],\n",
       "                         'xgboost__min_child_weight': [11],\n",
       "                         'xgboost__n_estimators': [100],\n",
       "                         'xgboost__objective': ['binary:logistic'],\n",
       "                         'xgboost__subsample': [0.8]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBooster = GridSearchCV(pipe_final, cv=3,\n",
    "                         n_jobs=1, param_grid={key:[value] for (key,value) in grid.best_params_.items()},\n",
    "                         verbose=1, scoring='roc_auc')\n",
    "\n",
    "XGBooster.fit(df_train.drop(columns=[\"TARGET\"]), df_train.TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Export Best Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCS.joblib']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBooster.best_estimator_\n",
    "joblib.dump(model, 'SCS.joblib', compress=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the saved model with joblib\n",
    "pipe = joblib.load('SCS.joblib')\n",
    "\n",
    "# apply the whole pipeline to data\n",
    "sum(pipe.predict(df_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
